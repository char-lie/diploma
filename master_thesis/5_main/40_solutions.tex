\section{Розв'язок}

\subsection{Дискретизація простору параметрів}

Дискретизуємо простір параметрів $X$.
Введемо невелике $\delta x$ таке,
щоб ймовірність того, що значення потрапляє в рамки гіперкубу
$\left[ x - \frac{\delta x}{2}; x + \frac{\delta x}{2} \right]$,
приблизно дорівнювала добутку щільності розподілу в його центрі
на його об'єм
\begin{equation*}
  \mathbb{P}\left( \xi \in \left[ x - \frac{\delta x}{2};
                                  x + \frac{\delta x}{2} \right] \right)
  \approx p\left( x \right) \cdot \delta x^n,
\end{equation*}
де
\begin{equation*}
  \left( x \pm \frac{\delta x}{2} \right)_i = x_i \pm \frac{\delta x}{2},\qquad
  i = 1..n.
\end{equation*}
Далі треба розбити простір на гіперкуби
зі стороною $\delta x$ і вважати,
що коли величина потрапляє на територію певного гіперкубу,
то вона дорівнює значенню в його центрі.

Відомо, що параметри $x$~---~набори коефіцієнтів нормованих головних компонент,
тобто мають стандартний ґаусовий розподіл.
Ймовірність того, що було згенеровано саме такий набір
\begin{equation*}
  \mathbb{P}_X\left( x \right)
  = \delta x^n \cdot \prod_{i=1}^n
    \frac{\exp{\left( - \frac{x_i^2}{2} \right)}}{\sqrt{2 \cdot \pi}}.
\end{equation*}

Введемо зручне позначення для шуму в пікселі $i$
\begin{equation*}
  y_i
  = t_i
    - f_i\left( x \right) \cdot \overline{h}_i\left( x \right)
    - \theta_i^B \cdot h_i\left( x \right),\qquad
    i \in I.
\end{equation*}
Розглядається одне зображення $t$.
З контексту буде зрозуміло, яке значення має $x$.
Ймовірність того, що обличчя на даному зображенні отримано з параметрами $x$
\begin{equation*}
  \mathbb{P}_Y\left( t \;\middle|\; x \right)
  = \delta x^{\left| I \right|} \cdot \prod_{i \in I}
    \frac{\exp{\left\{- \frac{y_i^2}
           {2 \cdot \sigma^2_t} \right\}}}
           {\sqrt{2 \cdot \pi \cdot \sigma^2_t}}.
\end{equation*}

Ймовірність того, що параметри $x$ приймають саме такі значення,
якщо відома оцінка координат опорних точок,
\begin{equation*}
  \mathbb{P}_L\left( x \right)
  = \delta x^{\left| L \right|} \cdot \prod_{l \in L}
      \frac{\exp{\left\{- \frac{\Delta_l^2\left( x \right)}
           {2 \cdot \sigma^2_L} \right\}}}
           {\sqrt{2 \cdot \pi \cdot \sigma^2_L}}.
\end{equation*}

Сумісна ймовірність зображення $t$ та параметрів $x$
\begin{equation*}
  \mathbb{P}\left( x, t \right)
  = \mathbb{P}_Y\left( t \;\middle|\; x \right)
    \cdot \mathbb{P}_X\left( x \right)
    \cdot \mathbb{P}_L\left( x \right)
\end{equation*}

\subsection{Інтервальна функція витрат}

Розглянемо функцію витрат,
за якої вірним є будь-який набір параметрів,
що знаходиться в $\delta$-околі дійсного набору $x$,
а за всі інші сплачується штраф $1$.

Для максимізації розглянемо логарифми ймовірностей,
тому що це зручніше,
ніж максимізація добутку кількох десятків тисяч або мільйонів значень
\begin{equation*}
  \begin{split}
    \mathbb{P}_X\left( x \right)
    &= \ln{\delta x^n} +
        \sum_{i = 1}^n
        \left\{
          - \frac{x_i^2}{2}
          - \frac{\ln{2} + \ln{\pi}}{2}
        \right\}, \\
    \mathbb{P}_Y\left( t \;\middle|\; x \right)
    &= \ln{\delta x^{\left| I \right|}} +
        \sum_{i \in I}
        \left\{
          - \frac{y_i^2}{2 \cdot \sigma^2_t}
          - \frac{\ln{2} + \ln{\pi} + \ln{\sigma^2_t}}{2}
        \right\}, \\
    \mathbb{P}_L\left( x \right)
    &= \ln{\delta x^{\left| L \right|}} +
        \sum_{l \in L}
        \left\{
          - \frac{\Delta_l^2\left( x \right)}{2 \cdot \sigma_L^2}
          - \frac{\ln{2} + \ln{\pi} + \ln{\sigma_L^2}}{2}
        \right\}.
  \end{split}
\end{equation*}
Приберемо константні доданки,
що не впливають на результат максимізації ймовірності
появи даного зображення $t$ з даними параметрами $x$
\begin{equation*}
  \sum_{i \in I}
    \left\{
      - \frac{y_i^2}{2 \cdot \sigma^2_t}
      - \frac{\ln{\sigma^2_t}}{2}
    \right\}
  +
  \sum_{l \in L}
    \left\{
      - \frac{\Delta_l^2\left( x \right)}{2 \cdot \sigma_L^2}
      - \frac{\ln{\sigma_L^2}}{2}
    \right\}
  - \sum_{i = 1}^n \frac{x_i^2}{2}
  \to \max\limits_{x \in X}.
\end{equation*}
Помножимо на $-2$ та винесемо логарифм дисперсії за знак суми
\begin{equation}\label{eq:minimize}
  \left| I \right| \cdot \ln{\sigma^2_t}
  + \left| L \right| \cdot \ln{\sigma_L^2}
  + \sum_{i \in I} \frac{y_i^2}{\sigma^2_t}
  + \sum_{l \in L} \frac{\Delta_l^2\left( x \right)}{\sigma_L^2}
  + \sum_{i = 1}^n x_i^2
  \to \min\limits_{x \in X}.
\end{equation}
Оскільки дисперсія є константним невідомим параметром,
можна спростити
\begin{equation}\label{eq:minimize}
  \sum_{i \in I} \frac{y_i^2}{\sigma^2_t}
  + \sum_{l \in L} \frac{\Delta_l^2\left( x \right)}{\sigma_L^2}
  + \sum_{i = 1}^n x_i^2
  \to \min\limits_{x \in X}.
\end{equation}
В загальному випадку зображення містить не тільки обличчя.
Генерація усіх можливих фонів для кожної моделі є недоцільною.
Потрібно отримати формулу,
яка не прив'язана до кожного пікселя вхідного зображення $t$.
Для цього треба брати середньоквадратичне відхилення шуму по тим пікселям,
де на згенерованому зображенні $f\left( x \right)$ знаходиться обличчя.
За умовою накладений шум є
набором незалежних випадкових однаково розподілених величин,
тому квадрат середньоквадратичного відхилення будь-якої їх сукупності
буде оптимальною оцінкою їх дисперсії \cite{BorovkovMS}.
Це означає, що квадрат середньоквадратичного відхилення шуму
на обличчі буде тим точнішою оцінкою дисперсії шуму,
чим більше місця займає обличчя на зображенні.
\begin{equation}\label{eq:energy:common}
  \sum_{i \in I} \frac{y_i^2}{\sigma^2_t}
  + \sum_{l \in L} \frac{\Delta_l^2\left( x \right)}{\sigma_L^2}
  + \sum_{i = 1}^n x_i^2
  \to \min\limits_{x \in X}.
\end{equation}

Найпростіша цільова функція~---~сума квадратів відхилень
між дійсним та синтезованим зображенням.
Доречна у випадку, коли відомо лише те,
що на зображення накладено ґаусовий шум,
а функція витрат бінарна і розглядається дискретний випадок
\begin{equation*}
  \sum_{i \in I} y_i^2
  \to \min\limits_{x \in X}.
\end{equation*}
Проте в даній задачі її застосовувати не можна.
Справа в тому,
що коли параметри моделі $x$ приймають завеликі абсолютні значення,
отримані зображення приймають гротескну форму
(рис. \ref{fig:interval:notre-dame})
або зовсім не схожі на обличчя людини
(рис. \ref{fig:interval:non-human}).
Тому доданок, що містить параметри $x$, необхідний у цільовій функції.
Суму квадратів цих параметрів також називають умовою регуляризації,
яка допомогає запобігти того стану моделі,
коли вона вже не описує об'єкт реального світу.
\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.4\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/notre-dame}
    \caption{Модель голови схожа на Квазімодо}
    \label{fig:interval:notre-dame}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/non-human}
    \caption{Модель голови не схожа на голову}
    \label{fig:interval:non-human}
  \end{subfigure}
  \caption{Приклади моделей з завеликими значеннями параметрів}
\end{figure}

\subsection{Різниця параметрів}

У даному підрозділі не будемо враховувати опорні точки,
щоб запобігти завеликих формул.
Проте отриманий результат не втрачає гнучкості
і до нього без проблем можна додати ймовірність $P_L$.

Ймовірність того,
що дане зображення $t$ було отримано саме з параметрами $x$,
є добутком розглянутих ймовірностей
\begin{equation*}
  \mathbb{P}\left( x, t \right)
  = \mathbb{P}_I\left( t \mcond x \right)
    \cdot \mathbb{P}_X\left( x \right)
  = \delta x^{n + \left| I \right|} \cdot \prod_{i \in I}
  \frac{\exp{\left\{ - \frac{y_i^2}
       {2 \cdot \sigma_t^2} \right\}}}{\sqrt{2 \cdot \pi \cdot \sigma_t^2}}
  \cdot
  \prod_{i=1}^n
  \frac{\exp{\left\{ - \frac{x_i^2}{2} \right\}}}{\sqrt{2 \cdot \pi}}.
\end{equation*}
Винесемо константи з добутків
\begin{equation*}
  \mathbb{P}\left( x, t \right)
  = \left( \frac{\delta x}{\sqrt{2 \cdot \pi}} \right)^{n + \left| I \right|}
    \cdot \prod_{i \in I}
    \frac{\exp{\left\{ - \frac{y_i^2}
         {2 \cdot \sigma_t^2} \right\}}}{\sqrt{\sigma_t^2}}
    \cdot
    \prod_{i=1}^n
    \exp{\left\{ - \frac{x_i^2}{2} \right\}}.
\end{equation*}
В даному випадку не можна замінити дисперсію на її оцінку,
бо для кожного $x$ вона буде різною,
через що інтеграл ймовірностей не буде дорівнювати одиниці,
що порушує умову, якій повинна задовольняти ймовірнісна міра
\cite{dorogovtsev:1989}.
Введемо константу
\begin{equation*}
  c_t = \frac{
      \left( 2 \cdot \pi \right)^{- \frac{n + \left| I \right|}{2}}
      \cdot \left( \sigma_t^2 \right)^{- \frac{\left| I \right|}{2}}
    }{\mathbb{P}\left( t \right)}
\end{equation*}
Ймовірність певного набору параметрів $x$ на даному зображенні $t$
\begin{equation*}
  \mathbb{P}\left( x \mcond t \right)
  = c_t \cdot \delta x^{n + \left| I \right|}
    \cdot \exp{\left\{ - \frac{y_i^2}{2 \cdot \sigma_t^2} \right\}}
    \cdot \exp{\left\{ - \frac{\left\| x \right\|^2}{2} \right\}}.
\end{equation*}
Стратегія розпізнавання
\begin{equation*}
  q^*_{\theta}\left( t \right)
  = c_t \cdot \delta x^{n + \left| I \right|}
    \cdot \sum_{x \in X}
      x
      \cdot \exp{\left\{ - \frac{y_i^2}{2 \cdot \sigma_t^2} \right\}}
      \cdot \exp{\left\{ - \frac{\left\| x \right\|^2}{2} \right\}}.
\end{equation*}
Щільність розподілу набору параметрів $x$ на даному зображенні $t$
\begin{equation*}
  p\left( x \mcond t \right)
  = c_t
    \cdot \exp{\left\{ - \frac{y_i^2}{2 \cdot \sigma_t^2} \right\}}
    \cdot \exp{\left\{ - \frac{\left\| x \right\|^2}{2} \right\}}.
\end{equation*}
Стратегія розпізнавання у неперервному випадку
\begin{equation*}
  q^*_{\theta}\left( t \right)
  = c_t
    \cdot \int\limits_{x \in X}
      x
      \cdot \exp{\left\{ - \frac{y_i^2}{2 \cdot \sigma_t^2} \right\}}
      \cdot \exp{\left\{ - \frac{\left\| x \right\|^2}{2} \right\}}
    d\,x.
\end{equation*}

\subsection{Попередні результати в контексті Баєсової теорії розпізнавання}

Тепер очевидно,
що у попередніх роботах використовувалася бінарна функція витрат.
Знову розглянему цільову функцію \eqref{eq:energy:face2face}
\begin{equation*}
  E\left( x, \theta \right)
  = \omega_c \cdot \sum_{i \in I' \subset I} \frac{y_i^2}{\left| I' \right|}
  + \omega_r \cdot \sum_{j = 1}^{n} x_j^2
  + \omega_l \cdot \sum_{l \in L} \frac{\Delta_l^2\left( x \right)}
                                       {\left| L \right|}
  \to \min\limits_{x \in X},
\end{equation*}
для якої коефіцієнти $\omega_c$, $\omega_r$ і $\omega_l$
обираються рівними $1$, $2.5 \cdot 10^{-5}$ і $10$ відповідно.
Завдяки \eqref{eq:energy:common} стає зрозуміло,
звідки беруться ваги $\omega$.
Треба лише замінити суму квадратів елементів шуму $y_i$
добутом розміру картинки $\left| I \right|$ зі
зміщеною оцінкою його дисперсії на пікселях з обличчям $I' \subset I$
\begin{equation*}
  \left| I \right| \cdot
    \sum_{i \in I' \subset I} \frac{y_i^2}{\left| I' \right| \cdot \sigma^2_t}
  + \left| L \right| \cdot
    \sum_{l \in L} \frac{\Delta_l^2\left( x \right)}
                        {\left| L \right| \cdot \sigma_L^2}
  + \sum_{i = 1}^n x_i^2
  \to \min\limits_{x \in X}.
\end{equation*}
Оскільки $\omega_c = 1$,
а ділення енергії на константу не змінює результату мінімізації,
можна помножити вираз \eqref{eq:energy:common}
на дисперсію шуму картинки та поділити на її розмір
\begin{equation*}
  \sum_{i \in I' \subset I} \frac{y_i^2}{\left| I' \right|}
  + \frac{\sigma^2_t}{\sigma_L^2} \cdot \frac{\left| L \right|}{\left| I \right|}
    \cdot \sum_{l \in L} \frac{\Delta_l^2\left( x \right)}{\left| L \right|}
  + \frac{\sigma^2_t}{\left| I \right|} \cdot \sum_{i = 1}^n x_i^2
  \to \min\limits_{x \in X}.
\end{equation*}
Маємо систему рівнянь
\begin{equation*}
  \begin{cases}
    \omega_r &= \frac{\sigma^2_t}{\left| I \right|}, \\
    \omega_l &= \frac{\sigma^2_t}{\sigma_L^2}
                \cdot \frac{\left| L \right|}{\left| I \right|},
  \end{cases}
  \implies
  \begin{cases}
    \sigma^2_t &= \omega_r \cdot \left| I \right|, \\
    \sigma_L^2 &= \frac{\omega_r}{\omega_l} \cdot \left| L \right|.
  \end{cases}
\end{equation*}
Підставимо значення $\omega_r = 2.5 \cdot 10^{-5}$ і $\omega_l = 10$
\begin{equation*}
  \begin{cases}
    \sigma^2_t &= 2.5 \cdot 10^{-5} \cdot \left| I \right|, \\
    \sigma_L^2 &= 2.5 \cdot 10^{-6} \cdot \left| L \right|.
  \end{cases}
\end{equation*}
Бачимо,
що дисперсія шуму лінійно зростає з площею зображення.
Для картинки $500 \times 500$ вона дорівнює одиниці,
а для одного мегапікселя це значення вже сягає $25$.
Для більших зображень сама модель голови має замалу точність,
тому немає сенсу розглядати поведінку дисперсії на них.
Дисперсія відхилення опорних точок від дійсного положення
лінійно залежить від їх кількості.
Точних даних немає,
проте використаний для знаходження особливих точок алгоритм
зазвичай демонструється з набором із $194$ опорних точок,
які було запропоновано у базі PUT Face \cite{put-face:2008}.
В такому випадку дисперсія дорівнює $4.85 \cdot 10^{-4}$,
тобто вважається, що оцінка положення точок ідеальна.

\subsection{Врахування фону}

Методи оптимізації використовують порівняння
значень цільової функції в різних точках
для визначення кращого набору параметрів
\begin{equation*}
  E\left( x^j \right) < E\left( x^{j+1} \right).
\end{equation*}
Це еквівалентно порівнянню їх різниці з нулем
\begin{equation*}
  E\left( x^j \right) - E\left( x^{j+1} \right) < 0.
\end{equation*}
Введемо позначення для тієї множини пікселів,
що на даній ітерації вважається множиною пікселів обличчя
\begin{equation*}
  I^j = \left\{ i \mcond i \in I, h_i = 0 \right\}.
\end{equation*}
Позначимо відому частину енергії, що не містить фону,
\begin{equation*}
  E'\left( x^j \right)
  = \sum_{i \in I_j} \frac{\left\| f\left( x^j \right) - t_i \right\|^2}{\sigma^2_t}
    + \sum_{l \in L} \frac{\Delta_l^2\left( x \right)}{\sigma_L^2}
    + \sum_{i = 1}^n x_i^2.
\end{equation*}
Частина, що містить фон
\begin{equation*}
  E^B\left( x^j \right)
  = \sum_{i \in I \setminus I_j} \frac{\left\| \theta_i^B - t_i \right\|^2}{\sigma^2_t}.
\end{equation*}
Цільова функція
\begin{equation*}
  E\left( x^j \right) = E'\left( x^j \right) + E^B\left( x^j \right).
\end{equation*}
Різниця цільових функцій приймає вигляд
\begin{equation*}
  E\left( x^j \right) - E\left( x^{j+1} \right)
  = E'\left( x^j \right) - E'\left( x^{j+1} \right)
    + E^B\left( x^j \right) - E^B\left( x^{j+1} \right)
  < 0.
\end{equation*}
Перенесемо різницю функцій, що містять фон, в праву частину нерівності
\begin{equation*}
  E'\left( x^j \right) - E'\left( x^{j+1} \right)
  < E^B\left( x^{j+1} \right) - E^B\left( x^j \right).
  = \sum_{i \in I \setminus I_{j+1}} \frac{\left\| \theta_i^B - t_i \right\|^2}{\sigma^2_t}
    - \sum_{i \in I \setminus I_j} \frac{\left\| \theta_i^B - t_i \right\|^2}{\sigma^2_t}.
\end{equation*}
Оскільки шум за умовою ґаусовий,
значення $E^B\left( x^j \right)$ є випадкова величина,
яка має розподіл $\chi^2$ з $\left| I \setminus I_j \right|$
ступенями вільності.
Порівняння приймає стохастичний характер
\begin{equation}\label{eq:background:comparison}
  \begin{cases}
    E'\left( x^j \right) - E'\left( x^{j+1} \right)
    < \chi \sim \zeta_1 - \zeta_2, \\
    \zeta_1 \sim \chi^2_{\left| I_j \setminus I_{j+1} \right|}, \\
    \zeta_2 \sim \chi^2_{\left| I_{j+1} \setminus I_j \right|}.
  \end{cases}
\end{equation}

Для того, щоб нерівність \eqref{eq:background:comparison}
виконувалася з ймовірністю $1$,
необхідно, щоб кожне наступне обличчя було не менше поточного,
доки не буде заповнено все зображення.
Альтернативний варіант~---~обмежити коливання силуету обличчя,
щоб різниця його площі на різних ітераціях була якнайменшою.

Оскільки не було декларовано, що зображення~---~прямокутна область,
вона може бути довільної форми.
Дане зауваження дозволяє комбінувати обидва міркування
щодо коректної стратегії пошуку вірної моделі.
Якщо заздалегіть за допомогою деякого алгоритму отримати
оцінку силуету обличчя $\hat{I'}$,
його можна використовувати в якості множини пікселів $I$.
Отримали задачу
\begin{equation*}
  \begin{cases}
    E\left( x \right) \to \min\limits_x, \\
    \left| I' \Delta \hat{I'} \right| < c, \qquad c > 0,
  \end{cases}
\end{equation*}
де $c$~---~наперед визначена точність оцінки силуету обличчя.

\subsection{Біометрична ідентифікація}

Використаємо скалярний добуток параметрів двох облич
з оригінальної статті про ідентифікацію за допомогою генеративної моделі обличчя
Базельского университету \cite{blanz:romdhani:vetter}
\begin{equation*}
  \left( x, x' \right) = \sum_{i=1}^n x_i \cdot x_i'.
\end{equation*}
Оскільки головні компоненти нормовані, ділення на дисперсію відсутнє,
бо вона дорівнює одиниці.
Такий скалярний добуток породжує відстань Махалонобіса \cite{Haykin:1998}
\begin{equation*}
  \left\| x \right\|^2 = \left( x, x \right) = \sum_{i=1}^n x_i^2.
\end{equation*}
Було доведено \cite{Moon:2017}, що краще використовувати аналог косинуса кута
між двома обличчями
\begin{equation*}
  d\left( x, x' \right)
  = \frac{\left( x, x' \right)}{\left\| x \right\| \cdot \left\| x' \right\|}.
\end{equation*}
