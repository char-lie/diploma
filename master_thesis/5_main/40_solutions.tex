\section{Розв'язок}

\vspace{-\baselineskip}

\subsection{Дискретизація простору параметрів}

Дискретизуємо простір параметрів $X$.
Введемо невелике $\delta x$ таке,
щоб ймовірність того, що значення потрапляє в рамки гіперкубу
$\left[ x - \frac{\delta x}{2}; x + \frac{\delta x}{2} \right]$,
приблизно дорівнювала добутку щільності розподілу в його центрі
на його об'єм
\begin{equation*}
  \mathbb{P}\left( \xi \in \left[ x - \frac{\delta x}{2};
                                  x + \frac{\delta x}{2} \right] \right)
  \approx p\left( x \right) \cdot \delta x^n,
\end{equation*}
де
\begin{equation*}
  \left( x \pm \frac{\delta x}{2} \right)_i = x_i \pm \frac{\delta x}{2},\qquad
  i = 1..n.
\end{equation*}
Далі треба розбити простір на гіперкуби
зі стороною $\delta x$ і вважати,
що коли величина потрапляє на територію певного гіперкубу,
то вона дорівнює значенню в його центрі.

Відомо, що параметри $x$~---~набори коефіцієнтів нормованих головних компонент,
тобто мають стандартний ґаусовий розподіл.
Ймовірність того, що було згенеровано саме такий набір
\begin{equation*}
  \mathbb{P}_X\left( x \right)
  = \delta x^n \cdot \prod_{i=1}^n
    \frac{\exp{\left( - \frac{x_i^2}{2} \right)}}{\sqrt{2 \cdot \pi}}.
\end{equation*}

Введемо зручне позначення для шуму в пікселі $i$
\begin{equation*}
  y_i
  = t_i
    - f_i\left( x \right) \cdot \overline{h}_i\left( x \right)
    - \theta_i^B \cdot h_i\left( x \right),\qquad
    i \in I.
\end{equation*}
Розглядається одне зображення $t$.
З контексту буде зрозуміло, яке значення має $x$.
Ймовірність того, що обличчя на даному зображенні отримано з параметрами $x$
\begin{equation*}
  \mathbb{P}_Y\left( t \;\middle|\; x \right)
  = \delta x^{\left| I \right|} \cdot \prod_{i \in I}
    \frac{\exp{\left\{- \frac{y_i^2}
           {2 \cdot \sigma^2_t} \right\}}}
           {\sqrt{2 \cdot \pi \cdot \sigma^2_t}}.
\end{equation*}

Ймовірність того, що параметри $x$ приймають саме такі значення,
якщо відома оцінка координат опорних точок,
\begin{equation*}
  \mathbb{P}_L\left( x \right)
  = \delta x^{\left| L \right|} \cdot \prod_{l \in L}
      \frac{\exp{\left\{- \frac{\Delta_l^2\left( x \right)}
           {2 \cdot \sigma^2_L} \right\}}}
           {\sqrt{2 \cdot \pi \cdot \sigma^2_L}}.
\end{equation*}

Сумісна ймовірність зображення $t$ та параметрів $x$
\begin{equation*}
  \mathbb{P}\left( x, t \right)
  = \mathbb{P}_Y\left( t \;\middle|\; x \right)
    \cdot \mathbb{P}_X\left( x \right)
    \cdot \mathbb{P}_L\left( x \right)
\end{equation*}

\subsection{Інтервальна функція витрат}

Розглянемо функцію витрат,
за якої вірним є будь-який набір параметрів,
що знаходиться в $\delta$-околі дійсного набору $x$,
а за всі інші сплачується штраф $1$.

Для максимізації розглянемо логарифми ймовірностей,
тому що це зручніше,
ніж максимізація добутку кількох десятків тисяч або мільйонів значень
\begin{equation*}
  \begin{split}
    \mathbb{P}_X\left( x \right)
    &= \ln{\delta x^n} +
        \sum_{i = 1}^n
        \left\{
          - \frac{x_i^2}{2}
          - \frac{\ln{2} + \ln{\pi}}{2}
        \right\}, \\
    \mathbb{P}_Y\left( t \;\middle|\; x \right)
    &= \ln{\delta x^{\left| I \right|}} +
        \sum_{i \in I}
        \left\{
          - \frac{y_i^2}{2 \cdot \sigma^2_t}
          - \frac{\ln{2} + \ln{\pi} + \ln{\sigma^2_t}}{2}
        \right\}, \\
    \mathbb{P}_L\left( x \right)
    &= \ln{\delta x^{\left| L \right|}} +
        \sum_{l \in L}
        \left\{
          - \frac{\Delta_l^2\left( x \right)}{2 \cdot \sigma_L^2}
          - \frac{\ln{2} + \ln{\pi} + \ln{\sigma_L^2}}{2}
        \right\}.
  \end{split}
\end{equation*}
Приберемо константні доданки,
що не впливають на результат максимізації ймовірності
появи даного зображення $t$ з даними параметрами $x$
\begin{equation*}
  \sum_{i \in I}
    \left\{
      - \frac{y_i^2}{2 \cdot \sigma^2_t}
      - \frac{\ln{\sigma^2_t}}{2}
    \right\}
  +
  \sum_{l \in L}
    \left\{
      - \frac{\Delta_l^2\left( x \right)}{2 \cdot \sigma_L^2}
      - \frac{\ln{\sigma_L^2}}{2}
    \right\}
  - \sum_{i = 1}^n \frac{x_i^2}{2}
  \to \max\limits_{x \in X}.
\end{equation*}
Помножимо на $-2$ та винесемо логарифм дисперсії за знак суми
\begin{equation*}
  \left| I \right| \cdot \ln{\sigma^2_t}
  + \left| L \right| \cdot \ln{\sigma_L^2}
  + \sum_{i \in I} \frac{y_i^2}{\sigma^2_t}
  + \sum_{l \in L} \frac{\Delta_l^2\left( x \right)}{\sigma_L^2}
  + \sum_{i = 1}^n x_i^2
  \to \min\limits_{x \in X}.
\end{equation*}
Оскільки дисперсія є константним невідомим параметром,
можна спростити
\begin{equation}\label{eq:minimize}
  \sum_{i \in I} \frac{y_i^2}{\sigma^2_t}
  + \sum_{l \in L} \frac{\Delta_l^2\left( x \right)}{\sigma_L^2}
  + \sum_{i = 1}^n x_i^2
  \to \min\limits_{x \in X}.
\end{equation}
В загальному випадку зображення містить не тільки обличчя.
Генерація усіх можливих фонів для кожної моделі є недоцільною.
Потрібно отримати формулу,
яка не прив'язана до кожного пікселя вхідного зображення $t$.
Для цього треба брати середньоквадратичне відхилення шуму по тим пікселям,
де на згенерованому зображенні $f\left( x \right)$ знаходиться обличчя.
За умовою накладений шум є
набором незалежних випадкових однаково розподілених величин,
тому квадрат середньоквадратичного відхилення будь-якої їх сукупності
буде оптимальною оцінкою їх дисперсії \cite{BorovkovMS}.
Це означає, що квадрат середньоквадратичного відхилення шуму
на обличчі буде тим точнішою оцінкою дисперсії шуму,
чим більше місця займає обличчя на зображенні.
\begin{equation}\label{eq:energy:common}
  \sum_{i \in I} \frac{y_i^2}{\sigma^2_t}
  + \sum_{l \in L} \frac{\Delta_l^2\left( x \right)}{\sigma_L^2}
  + \sum_{i = 1}^n x_i^2
  \to \min\limits_{x \in X}.
\end{equation}

Найпростіша цільова функція~---~сума квадратів відхилень
між дійсним та синтезованим зображенням.
Доречна у випадку, коли відомо лише те,
що на зображення накладено ґаусовий шум,
а функція витрат бінарна і розглядається дискретний випадок
\begin{equation*}
  \sum_{i \in I} y_i^2
  \to \min\limits_{x \in X}.
\end{equation*}
Проте в даній задачі її застосовувати не можна.
Справа в тому,
що коли параметри моделі $x$ приймають завеликі абсолютні значення,
отримані зображення приймають гротескну форму
(рис. \ref{fig:interval:notre-dame})
або зовсім не схожі на обличчя людини
(рис. \ref{fig:interval:non-human}).
Тому доданок, що містить параметри $x$, необхідний у цільовій функції.
Суму квадратів цих параметрів також називають умовою регуляризації,
яка допомогає запобігти того стану моделі,
коли вона вже не описує об'єкт реального світу.
\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.4\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/notre-dame}
    \caption{Модель голови схожа на Квазімодо}
    \label{fig:interval:notre-dame}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/non-human}
    \caption{Модель голови не схожа на голову}
    \label{fig:interval:non-human}
  \end{subfigure}
  \caption{Приклади моделей з завеликими значеннями параметрів}
\end{figure}

\subsection{Різниця параметрів}

У даному підрозділі не будемо враховувати опорні точки,
щоб запобігти завеликих формул.
Проте отриманий результат не втрачає гнучкості
і до нього без проблем можна додати ймовірність $P_L$.

Ймовірність того,
що дане зображення $t$ було отримано саме з параметрами $x$,
є добутком розглянутих ймовірностей
\begin{equation*}
  \mathbb{P}\left( x, t \right)
  = \mathbb{P}_I\left( t \mcond x \right)
    \cdot \mathbb{P}_X\left( x \right)
  = \delta x^{n + \left| I \right|} \cdot \prod_{i \in I}
  \frac{\exp{\left\{ - \frac{y_i^2}
       {2 \cdot \sigma_t^2} \right\}}}{\sqrt{2 \cdot \pi \cdot \sigma_t^2}}
  \cdot
  \prod_{i=1}^n
  \frac{\exp{\left\{ - \frac{x_i^2}{2} \right\}}}{\sqrt{2 \cdot \pi}}.
\end{equation*}
Винесемо константи з добутків
\begin{equation*}
  \mathbb{P}\left( x, t \right)
  = \left( \frac{\delta x}{\sqrt{2 \cdot \pi}} \right)^{n + \left| I \right|}
    \cdot \prod_{i \in I}
    \frac{\exp{\left\{ - \frac{y_i^2}
         {2 \cdot \sigma_t^2} \right\}}}{\sqrt{\sigma_t^2}}
    \cdot
    \prod_{i=1}^n
    \exp{\left\{ - \frac{x_i^2}{2} \right\}}.
\end{equation*}
В даному випадку не можна замінити дисперсію на її оцінку,
бо для кожного $x$ вона буде різною,
через що інтеграл ймовірностей не буде дорівнювати одиниці,
що порушує умову, якій повинна задовольняти ймовірнісна міра
\cite{dorogovtsev:1989}.
Введемо константу
\begin{equation*}
  c_t = \frac{
      \left( 2 \cdot \pi \right)^{- \frac{n + \left| I \right|}{2}}
      \cdot \left( \sigma_t^2 \right)^{- \frac{\left| I \right|}{2}}
    }{\mathbb{P}\left( t \right)}
\end{equation*}
Ймовірність певного набору параметрів $x$ на даному зображенні $t$
\begin{equation*}
  \mathbb{P}\left( x \mcond t \right)
  = c_t \cdot \delta x^{n + \left| I \right|}
    \cdot \exp{\left\{ - \frac{y_i^2}{2 \cdot \sigma_t^2} \right\}}
    \cdot \exp{\left\{ - \frac{\left\| x \right\|^2}{2} \right\}}.
\end{equation*}
Стратегія розпізнавання
\begin{equation*}
  q^*_{\theta}\left( t \right)
  = c_t \cdot \delta x^{n + \left| I \right|}
    \cdot \sum_{x \in X}
      x
      \cdot \exp{\left\{ - \frac{y_i^2}{2 \cdot \sigma_t^2} \right\}}
      \cdot \exp{\left\{ - \frac{\left\| x \right\|^2}{2} \right\}}.
\end{equation*}
Щільність розподілу набору параметрів $x$ на даному зображенні $t$
\begin{equation*}
  p\left( x \mcond t \right)
  = c_t
    \cdot \exp{\left\{ - \frac{y_i^2}{2 \cdot \sigma_t^2} \right\}}
    \cdot \exp{\left\{ - \frac{\left\| x \right\|^2}{2} \right\}}.
\end{equation*}
Стратегія розпізнавання у неперервному випадку
\begin{equation*}
  q^*_{\theta}\left( t \right)
  = c_t
    \cdot \int\limits_{x \in X}
      x
      \cdot \exp{\left\{ - \frac{y_i^2}{2 \cdot \sigma_t^2} \right\}}
      \cdot \exp{\left\{ - \frac{\left\| x \right\|^2}{2} \right\}}
    d\,x.
\end{equation*}

\subsection{Попередні результати в контексті баєсової теорії розпізнавання}

Тепер очевидно,
що у попередніх роботах використовувалася бінарна функція витрат.
Знову розглянему цільову функцію \eqref{eq:energy:face2face}
\begin{equation*}
  E\left( x, \theta \right)
  = \omega_c \cdot \sum_{i \in I' \subset I} \frac{y_i^2}{\left| I' \right|}
  + \omega_r \cdot \sum_{j = 1}^{n} x_j^2
  + \omega_l \cdot \sum_{l \in L} \frac{\Delta_l^2\left( x \right)}
                                       {\left| L \right|}
  \to \min\limits_{x \in X},
\end{equation*}
для якої коефіцієнти $\omega_c$, $\omega_r$ і $\omega_l$
обираються рівними $1$, $2.5 \cdot 10^{-5}$ і $10$ відповідно.
Завдяки \eqref{eq:energy:common} стає зрозуміло,
звідки беруться ваги $\omega$.
Треба лише замінити суму квадратів елементів шуму $y_i$
добутком розміру картинки $\left| I \right|$ зі
зміщеною оцінкою його дисперсії на пікселях з обличчям $I' \subset I$
\begin{equation*}
  \left| I \right| \cdot
    \sum_{i \in I' \subset I} \frac{y_i^2}{\left| I' \right| \cdot \sigma^2_t}
  + \left| L \right| \cdot
    \sum_{l \in L} \frac{\Delta_l^2\left( x \right)}
                        {\left| L \right| \cdot \sigma_L^2}
  + \sum_{i = 1}^n x_i^2
  \to \min\limits_{x \in X}.
\end{equation*}
Оскільки $\omega_c = 1$,
а ділення енергії на константу не змінює результату мінімізації,
можна помножити вираз \eqref{eq:energy:common}
на дисперсію шуму картинки та поділити на її розмір
\begin{equation*}
  \sum_{i \in I' \subset I} \frac{y_i^2}{\left| I' \right|}
  + \frac{\sigma^2_t}{\sigma_L^2} \cdot \frac{\left| L \right|}{\left| I \right|}
    \cdot \sum_{l \in L} \frac{\Delta_l^2\left( x \right)}{\left| L \right|}
  + \frac{\sigma^2_t}{\left| I \right|} \cdot \sum_{i = 1}^n x_i^2
  \to \min\limits_{x \in X}.
\end{equation*}
Маємо систему рівнянь
\begin{equation*}
  \begin{cases}
    \omega_r &= \frac{\sigma^2_t}{\left| I \right|}, \\
    \omega_l &= \frac{\sigma^2_t}{\sigma_L^2}
                \cdot \frac{\left| L \right|}{\left| I \right|},
  \end{cases}
  \implies
  \begin{cases}
    \sigma^2_t &= \omega_r \cdot \left| I \right|, \\
    \sigma_L^2 &= \frac{\omega_r}{\omega_l} \cdot \left| L \right|.
  \end{cases}
\end{equation*}
Підставимо значення $\omega_r = 2.5 \cdot 10^{-5}$ і $\omega_l = 10$
\begin{equation*}
  \begin{cases}
    \sigma^2_t &= 2.5 \cdot 10^{-5} \cdot \left| I \right|, \\
    \sigma_L^2 &= 2.5 \cdot 10^{-6} \cdot \left| L \right|.
  \end{cases}
\end{equation*}
Бачимо,
що дисперсія шуму лінійно зростає з площею зображення.
Для картинки $500 \times 500$ вона дорівнює одиниці,
а для одного мегапікселя це значення вже сягає $25$.
Для більших зображень сама модель голови має замалу точність,
тому немає сенсу розглядати поведінку дисперсії на них.
Дисперсія відхилення опорних точок від дійсного положення
лінійно залежить від їх кількості.
Точних даних немає,
проте використаний для знаходження особливих точок алгоритм
зазвичай демонструється з набором із $194$ опорних точок,
які було запропоновано у базі PUT Face \cite{put-face:2008}.
В такому випадку дисперсія дорівнює $4.85 \cdot 10^{-4}$,
тобто вважається, що оцінка положення точок ідеальна.

\subsection{Врахування фону}

Методи оптимізації використовують порівняння
значень цільової функції в різних точках
для визначення кращого набору параметрів.
Оскільки в досліджуваній задачі
йдеться мова саме про мінімізацію певної функції $E$,
значення на наступному кроці $j+1$
повинно бути меншим за значення на поточному кроці $j$
\begin{equation*}
  E\left( x^j \right) > E\left( x^{j+1} \right).
\end{equation*}
Введемо позначення для тієї множини пікселів,
що на даній ітерації вважається множиною пікселів обличчя
\begin{equation*}
  I^j = \left\{ i \mcond i \in I, h_i\left( x^j \right) = 0 \right\}.
\end{equation*}
Відому частину енергії, що не містить фону, позначимо
\begin{equation*}
  E'\left( x^j \right)
  = \sum_{i \in I_j} \frac{\left\| f\left( x^j \right) - t_i \right\|^2}{\sigma^2_t}
    + \sum_{l \in L} \frac{\Delta_l^2\left( x \right)}{\sigma_L^2}
    + \sum_{i = 1}^n x_i^2.
\end{equation*}
Частину, що містить фон, позначимо
\begin{equation*}
  E^B\left( x^j \right)
  = \sum_{i \in I \setminus I_j} \frac{\left\| \theta_i^B - t_i \right\|^2}{\sigma^2_t}.
\end{equation*}
Цільова функція
\begin{equation*}
  E\left( x^j \right) = E'\left( x^j \right) + E^B\left( x^j \right).
\end{equation*}
Порівняння цільової функції на сусідніх кроках оптимізації приймає вигляд
\begin{equation*}
  E'\left( x^j \right) + E^B\left( x^j \right)
  > E'\left( x^{j+1} \right) + E^B\left( x^{j+1} \right).
\end{equation*}
Перенесемо функції, що містять фон, в праву частину нерівності,
а відомі в ліву
\begin{equation}\label{eq:energy:comparison}
  E'\left( x^j \right) - E'\left( x^{j+1} \right)
  > E^B\left( x^{j+1} \right) - E^B\left( x^j \right).
\end{equation}
Суми з правої сторони нерівності містять невідомі величини,
проте за умовою фон на одному зображенні не змінюється,
тому $\theta^B_i$ має одне й те саме значення на будь-якій ітерації.
Оскільки
\begin{equation*}
  \left( I \setminus I_{j+1} \right) \cap \left( I \setminus I_j \right)
  = \overline{I_{j+1}} \cap \overline{I_j}
  = \overline{I_{j+1} \cup I_j}
\end{equation*}
і
\begin{equation*}
  \begin{cases}
    \overline{I_{j+1}} \setminus \overline{I_{j+1} \cup I_j}
    = \overline{I_{j+1}} \cap \left( I_{j+1} \cup I_j \right)
    = I_{j} \setminus I_{j+1}, \\
    \overline{I_{j}} \setminus \overline{I_{j+1} \cup I_j}
    = \overline{I_{j}} \cap \left( I_{j+1} \cup I_j \right)
    = I_{j+1} \setminus I_{j},
  \end{cases}
\end{equation*}
маємо
\begin{equation*}
  E^B\left( x^{j+1} \right) - E^B\left( x^j \right)
  = \sum_{i \in I_j \setminus I_{j+1}} \frac{\left\| \theta_i^B - t_i \right\|^2}{\sigma^2_t}
    - \sum_{i \in I_{j+1} \setminus I_j} \frac{\left\| \theta_i^B - t_i \right\|^2}{\sigma^2_t}.
\end{equation*}
Нерівність \eqref{eq:energy:comparison} приймає вигляд
\begin{equation*}
  E'\left( x^j \right) - E'\left( x^{j+1} \right)
  > \sum_{i \in I_j \setminus I_{j+1}} \frac{\left\| \theta_i^B - t_i \right\|^2}{\sigma^2_t}
    - \sum_{i \in I_{j+1} \setminus I_j} \frac{\left\| \theta_i^B - t_i \right\|^2}{\sigma^2_t}.
\end{equation*}
Згадаємо, що за умовою на зображення накладено
центрований адитивний ґаусовий шум з дисперсією $\sigma^2_t$
\begin{equation*}
  E'\left( x^j \right) - E'\left( x^{j+1} \right)
  > \sum_{i \in I_j \setminus I_{j+1}} \frac{\eta_i^2}{\sigma^2_t}
    - \sum_{i \in I_{j+1} \setminus I_j} \frac{\eta_i^2}{\sigma^2_t}.
\end{equation*}
Суму квадратів незалежних випадкових величин,
що мають стандартний нормальний розподіл,
розподілено за законом $\chi^2$
\begin{equation}\label{eq:background:comparison}
  \begin{cases}
    E'\left( x^j \right) - E'\left( x^{j+1} \right)
    > \chi \sim \zeta_1 - \zeta_2, \\
    \zeta_1 \sim \chi^2_{\left| I_j \setminus I_{j+1} \right|}, \\
    \zeta_2 \sim \chi^2_{\left| I_{j+1} \setminus I_j \right|}.
  \end{cases}
\end{equation}

Для того, щоб нерівність \eqref{eq:background:comparison}
виконувалася з ймовірністю $1$,
коли $E'\left( x^j \right) > E'\left( x^{j+1} \right)$,
необхідно, щоб кожне наступне обличчя було не менше поточного,
доки не буде заповнено все зображення.
Альтернативний варіант~---~обмежити коливання силуету обличчя,
щоб різниця його площі на різних ітераціях була якнайменшою,
завдяки чому ймовірність збереження нерівності буде якнайбільшою.

Оскільки не було декларовано, що зображення~---~прямокутна область,
вона може бути довільної форми.
Дане зауваження дозволяє комбінувати обидва міркування
щодо коректної стратегії пошуку вірної моделі.
Якщо заздалегідь за допомогою деякого алгоритму отримати
оцінку силуету обличчя $\hat{I'}$,
його можна використовувати в якості множини пікселів $I$.
Отримали задачу
\begin{equation*}
  \begin{cases}
    E\left( x \right) \to \min\limits_x, \\
    \left| I' \Delta \hat{I'} \right| < c, \qquad c > 0,
  \end{cases}
\end{equation*}
де $c$~---~наперед визначена точність оцінки силуету обличчя.

\subsection{Біометрична ідентифікація}

У статті дослідників, що працювали над створенням базельскої моделі обличчя,
вказано метод ідентифікації людей з використанням реконструкцій
просторових конфігурацій їх облич,
який дає добрі експериментальні результати \cite{blanz:romdhani:vetter}.

Вводиться скалярний добуток параметрів двох моделей обличчя
\begin{equation*}
  \left( x, x' \right) = \sum_{i=1}^n x_i \cdot x_i'.
\end{equation*}
Оскільки головні компоненти нормовані,
вони мають одиничну дисперсію,
ділення на яку не впливає на результат.
Такий скалярний добуток породжує відстань Махаланобіса \cite{Haykin:1998},
що в даному випадку еквівалентна звичайній евклідовій нормі
\begin{equation*}
  \left\| x \right\|^2 = \left( x, x \right) = \sum_{i=1}^n x_i^2.
\end{equation*}
Було доведено \cite{Moon:2017},
що для ідентифікації краще використовувати аналог косинуса кута
між двома обличчями в якості міри несхожості
\begin{equation*}
  d\left( x, x' \right)
  = \frac{\left( x, x' \right)}{\left\| x \right\| \cdot \left\| x' \right\|}.
\end{equation*}
