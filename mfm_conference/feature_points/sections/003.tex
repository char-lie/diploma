\section{Problem of finding affine transformation by projection}

\subsection{Statement of the general problem}

Given projection of vertices on $xy$ plane,
we can build a system of bilinear equations
\begin{equation*}
  p_{v_i}
  = G_{v_i}\left( \pmb{s}; A \right)
  = \sum\limits_{j = 0}^{n} \sum\limits_{k = 0}^{3}
    s_{j} \cdot \lambda^{j}_{v_k} \cdot a_i^{k},
  \quad i \in \mathcal{I}^p.
\end{equation*}
Assume we have added projection of another vertex $u$.
Coefficients $s$ and $A^x$ are generic
for entire model~---~thus for all vertices.
We need to add only PCA coefficients
\begin{equation*}
  p_{v_i} + p_{u_i}
  = \sum\limits_{j = 0}^{n} \sum\limits_{k = 0}^{3}
    s_{j} \cdot \left( \lambda^{j}_{v_k} + \lambda^{j}_{u_k} \right)
    \cdot a_x^{k},
  \quad i \in \mathcal{I}.
\end{equation*}
Matrix notation
\begin{equation*}
  G_{v_i}\left( \pmb{s}; A \right) + G_{u_i}\left( \pmb{s}; A \right)
  = \pmb{a}_i \cdot \left( \Lambda_v + \Lambda_u \right) \cdot \pmb{s},
  \quad i \in \mathcal{I}.
\end{equation*}
We can multiply projections by scalars
and this will change only coefficients matrix.
In other words, for any real column vector $\pmb{\varphi}$
\begin{equation*}
  \sum_{w \in V} \varphi_w \cdot p_{w_i}
  = \pmb{a}_i
    \cdot \left( \sum_{w \in V} \varphi_w \cdot \Lambda_w \right)
    \cdot \pmb{s},
  \quad \pmb{\varphi} \in \mathbb{R}^m,
\end{equation*}
and it's possible to add equations with each other
and multiply them by constant.

We have a system of bilinear equations,
where only the coefficients matrix $\Lambda$ is known
\begin{equation}\label{eq:bilinear:matrix}
  p_{v_i}
  = \pmb{a}_i \cdot \Lambda_v \cdot \pmb{s},
  \quad i \in \mathcal{I}^p.
\end{equation}

\subsection{Reducing the problem to system of linear equations}
The first component of the shape vector $s$ equals $1$ by definition.
Other components are unknown,
but can be ignored if corresponding coefficients will be $0$.
We can reach this by finding such vector $\pmb{\varphi}^k$
that all cells except one in the row $k$ of the column $0$ will become zero
\begin{equation*}
  \left\{\begin{aligned}
    \sum\limits_{w \in V} \varphi_w^k \cdot \lambda_{w_k}^0 &\neq 0, \\
    \sum\limits_{w \in V} \varphi_w^k \cdot \lambda_{w_i}^j &= 0,
      \quad j > 0,
      \quad v_i \neq u_k, \\
  \end{aligned}\right.
  \quad i, k \in \mathcal{I}.
\end{equation*}
Given vector $\pmb{\varphi}^x$, we have an equation
\begin{equation*}
  \sum\limits_{w \in V} \varphi_w^x \cdot p_{w_x}
  = \pmb{a}_x
    \cdot \begin{bmatrix}
      0                                     & 0 & \dots & 0 \\
      \sum\limits_{w \in V} \varphi_w^x
        \cdot \lambda^0_{w_x} & 0 & \dots   & 0 \\
      0                                     & 0 & \dots & 0 \\
      0                                     & 0 & \dots & 0 \\
    \end{bmatrix}
    \cdot \pmb{s}.
\end{equation*}
Product of last two items will give a column vector
\begin{equation*}
  \sum\limits_{w \in V} \varphi_w^x \cdot p_{w_x}
  = \left\langle a^0_x, a_x^x, a_x^y, a_x^z \right\rangle
    \cdot \begin{bmatrix}
      0 \\
      \sum\limits_{w \in V} \varphi_w^x
        \cdot \lambda^0_{w_x}               \\
      0                                     \\
      0                                     \\
    \end{bmatrix}.
\end{equation*}
The final expression is
\begin{equation*}
  a_x^x
  = \frac{\sum\limits_{w \in V} \varphi_w^x \cdot p_{w_x}}
         {\sum\limits_{w \in V} \varphi_w^x \cdot \lambda^0_{w_x}}.
\end{equation*}
We should notice that only vector $\pmb{\varphi}^0$
should not have a zero sum of its components.
Other vectors should in order to have cell in the $0$th column and $0$th row
of the summary matrix be equal to zero.
This happens because in each $\Lambda_v$ matrix value in cell on this position
is equal to $1$.
Look at the following equation to see what it means for $\pmb{\varphi}^0$
and projection to $x$ axis
\begin{equation*}
  \sum\limits_{w \in V} \varphi_w^0 \cdot p_{w_x}
  = \pmb{a}_x \cdot \begin{bmatrix}
      \sum\limits_{w \in V} \varphi_w^0 & 0 & \dots & 0 \\
      0                                 & 0 & \dots   & 0 \\
      0                                 & 0 & \dots & 0 \\
      0                                 & 0 & \dots & 0 \\
    \end{bmatrix}
    \cdot \pmb{s}.
\end{equation*}
Solution for the $x$ offset is
\begin{equation*}
  a^0_x
  = \frac{\sum\limits_{w \in V} \varphi_w^0 \cdot p_{w_x}}
         {\sum\limits_{w \in V} \varphi_w^0}.
\end{equation*}

Now we have a system of linear equations
with respect to $x$ and $y$ rows of affine transformation matrix
\begin{equation*}
  \sum\limits_{w \in V} \varphi_w^j \cdot p_{w_i}
  = a_i^j \cdot \sum\limits_{w \in V} \varphi_w^j \lambda^0_{w_j},
  \quad i \in \mathcal{I}^p,
  \quad j \in \mathcal{I}^h.
\end{equation*}

\subsection{Solving reduced linear equations}

Vectorized matrices $\pmb{\Lambda}_v$
can be concatenated to matrix
in order to build a linear equation.
Example of the equation with respect to $\pmb{\varphi}^y$
\begin{equation}\label{eq:linear:affine}
  \begin{bmatrix}
    1                 & 1                 & \dots  & 1                 & \\
    \lambda^0_{v^1_x} & \lambda^0_{v^2_x} & \dots  & \lambda^0_{v^m_x} & \\
    \lambda^0_{v^1_y} & \lambda^0_{v^2_y} & \dots  & \lambda^0_{v^m_y} & \\
    \lambda^0_{v^1_z} & \lambda^0_{v^2_z} & \dots  & \lambda^0_{v^m_z} & \\
    \vdots            & \vdots            & \ddots & \vdots            & \\
    \lambda^n_{v^1_y} & \lambda^n_{v^2_y} & \dots  & \lambda^n_{v^m_y} & \\
    \lambda^n_{v^1_z} & \lambda^n_{v^2_z} & \dots  & \lambda^n_{v^m_z} & \\
  \end{bmatrix}
  \cdot
  \begin{bmatrix}
    \varphi^y_{v^1} \\
    \varphi^y_{v^2} \\
    \vdots          \\
    \varphi^y_{v^m} \\
  \end{bmatrix}
  =
  \begin{bmatrix}
    0       \\
    0       \\
    r_y     \\
    0       \\
    \vdots  \\
    0       \\
  \end{bmatrix}.
\end{equation}
where $r_y$ is a non-zero real number.
How to solve an equation if we don't know the value of $r_y$?
We can solve an equation which will give us a range of possible values of $r_y$
\begin{equation*}
  \begin{bmatrix}
    \lambda^0_{v^1_y} & \lambda^0_{v^2_y} & \dots & \lambda^0_{v^m_y} & \\
  \end{bmatrix}
  \cdot
  \begin{bmatrix}
    \varphi^y_{v^1} \\
    \varphi^y_{v^2} \\
    \vdots          \\
    \varphi^y_{v^m} \\
  \end{bmatrix}
  \neq 0,
\end{equation*}
and see what constraints vector $\pmb{\varphi}^y$ has.
Then the row which contains $\lambda^0_{v_y}$ coefficients
can be dropped from the original equation \eqref{eq:linear:affine}
and it can be solved as an ordinary linear equation
\begin{equation*}
  \begin{bmatrix}
    1                 & 1                 & \dots  & 1                 & \\
    \lambda^0_{v^1_x} & \lambda^0_{v^2_x} & \dots  & \lambda^0_{v^m_x} & \\
    \lambda^0_{v^1_z} & \lambda^0_{v^2_z} & \dots  & \lambda^0_{v^m_z} & \\
    \vdots            & \vdots            & \ddots & \vdots            & \\
    \lambda^n_{v^1_y} & \lambda^n_{v^2_y} & \dots  & \lambda^n_{v^m_y} & \\
    \lambda^n_{v^1_z} & \lambda^n_{v^2_z} & \dots  & \lambda^n_{v^m_z} & \\
  \end{bmatrix}
  \cdot
  \begin{bmatrix}
    \varphi^y_{v^1} \\
    \varphi^y_{v^2} \\
    \vdots          \\
    \varphi^y_{v^m} \\
  \end{bmatrix}
  =
  \begin{bmatrix}
    0       \\
    0       \\
    0       \\
    \vdots  \\
    0       \\
  \end{bmatrix}.
\end{equation*}
Intersection of solution of this equation
with constraints from the previous step
will be a final solution set, which may be empty.

When the system will be solved,
we will get two rows of the matrix $A$
and this will change bilinear equation \eqref{eq:bilinear:matrix}
to linear with respect to shape parameter $\pmb{s}$.
