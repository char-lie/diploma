\section{Problem of finding affine transformation by projection}

\subsection{Statement of the general problem}

Given projection of vertices on $xy$ plane,
we can build a system of bilinear equations
\begin{equation*}
  p_{v_i}
  = G_{v_i}\left( s; A \right)
  = \sum\limits_{j = 0}^{n} \sum\limits_{k = 0}^{3}
    s_{j} \cdot \lambda^{j}_{v_k} \cdot A_i^{k},
  \quad i \in I^p.
\end{equation*}
Assume we have added projection of another vertex $u$.
Coefficients $s$ and $A^x$ are generic
for entire model~---~thus for all vertices.
We need to add only PCA coefficients
\begin{equation*}
  p_{v_i} + p_{u_i}
  = \sum\limits_{j = 0}^{n} \sum\limits_{k = 0}^{3}
    s_{j} \cdot \left( \lambda^{j}_{v_k} + \lambda^{j}_{u_k} \right)
    \cdot A_x^{k},
  \quad i \in I.
\end{equation*}
In matrix view
\begin{equation*}
  G_{v_i}\left( s; A \right) + G_{u_i}\left( s; A \right)
  = A_i \cdot \left( \Lambda_v + \Lambda_u \right) \cdot s,
  \quad v, u \in V,
  \quad i \in I.
\end{equation*}
We can multiply projections by scalars
and this will change only coefficients matrix
\begin{equation*}
  \alpha \cdot p_{v_i} + \beta \cdot p_{u_i}
  = A_i
    \cdot \left( \alpha \cdot \Lambda_v + \beta \cdot \Lambda_u \right)
    \cdot s,
  \quad i \in I,
  \quad \alpha, \beta \in \mathbb{R}.
\end{equation*}
In other words, for any real column vector $\varphi$
\begin{equation*}
  \sum_{w \in V} \varphi_w \cdot p_{w_i}
  = A_i
    \cdot \left( \sum_{w \in V} \varphi_w \cdot \Lambda_w \right)
    \cdot s,
  \quad \varphi \in \mathbb{R}^m.
\end{equation*}

We have a system of bilinear equations,
where only the coefficients matrix $\Lambda$ is known
\begin{equation}\label{eq:bilinear:matrix}
  p_{v_i}
  = A_i \cdot \Lambda_v \cdot s,
  \quad i \in I^p.
\end{equation}

\subsection{Reducing the problem to system of linear equations}
The first component of the shape vector $s$ equals $1$ by definition.
Other components are unknown,
but can be ignored if corresponding coefficients will be $0$.
We can reach this by finding such vector $\varphi^k$
that all cells except one in the row $k$ of the column $0$ will become zero
\begin{equation*}
  \begin{cases}
    \sum\limits_{w \in V} \varphi_w^k \cdot \lambda_{w_k}^0 &\neq 0, \\
    \sum\limits_{w \in V} \varphi_w^k \cdot \lambda_{w_i}^j &= 0,
      \quad j > 0,
      \quad v_i \neq u_k, \\
  \end{cases}
  \quad i, k \in I.
\end{equation*}
Given vector $\varphi^x$, we have an equation
\begin{equation*}
  \sum\limits_{w \in V} \varphi_w^x \cdot p_{w_x}
  = A_x
    \cdot \begin{bmatrix}
      0                                     & 0 & \dots & 0 \\
      \sum\limits_{w \in V} \varphi_w^x
        \cdot \lambda^0_{w_x} & 0 & \dots   & 0 \\
      0                                     & 0 & \dots & 0 \\
      0                                     & 0 & \dots & 0 \\
    \end{bmatrix}
    \cdot \begin{bmatrix}
      1 \\
      s_1 \\
      \vdots \\
      s_n
    \end{bmatrix}.
\end{equation*}
Product of last two items will give a column vector
\begin{equation*}
  \sum\limits_{w \in V} \varphi_w^x \cdot p_{w_x}
  = \left\langle a^0_x, a_x^x, a_x^y, a_x^z \right\rangle
    \cdot \begin{bmatrix}
      0 \\
      \sum\limits_{w \in V} \varphi_w^x
        \cdot \lambda^0_{w_x}               \\
      0                                     \\
      0                                     \\
    \end{bmatrix}.
\end{equation*}
The final expression is
\begin{equation*}
  \sum\limits_{w \in V} \varphi_w^x \cdot p_{w_x}
  = a_x^x \cdot \sum\limits_{w \in V} \varphi_w^x \cdot \lambda^0_{w_x}.
\end{equation*}
We should notice that only vector $\varphi^0$
should not have a zero sum of its components.
Other vectors should in order to have cell in the $0$th column and $0$th row
of the summary matrix be equal to zero.
This happens because in each $\Lambda_v$ matrix value in cell on this position
is equal to $1$.
Look at the following equation to see what it means for $\varphi^0$
and projection to $x$ axis
\begin{equation*}
  \sum\limits_{w \in V} \varphi_w^0 \cdot p_{w_x}
  = \left\langle a^0_x, a_x^x, a_x^y, a_x^z \right\rangle
    \cdot \begin{bmatrix}
      \sum\limits_{w \in V} \varphi_w^0 & 0 & \dots & 0 \\
      0                                 & 0 & \dots   & 0 \\
      0                                 & 0 & \dots & 0 \\
      0                                 & 0 & \dots & 0 \\
    \end{bmatrix}
    \cdot \begin{bmatrix}
      1 \\
      s_1 \\
      \vdots \\
      s_n
    \end{bmatrix}.
\end{equation*}
Resulting equation for the $x$ offset is
\begin{equation*}
  \sum\limits_{w \in V} \varphi_w^0 \cdot p_{w_x}
  = a^0_x \cdot \sum\limits_{w \in V} \varphi_w^0.
\end{equation*}

Now we have a system of linear equations
with respect to $x$ and $y$ rows of affine transformation matrix
\begin{equation*}
  \sum\limits_{w \in V} \varphi_w^j \cdot p_{w_i}
  = A_i^j \cdot \sum\limits_{w \in V} \varphi_w^j \lambda^0_{w_j},
  \quad i \in I^p,
  \quad j \in I^h.
\end{equation*}
Even though $\Lambda_v$ are matrices,
we can represent them as column vectors
in order to find right vectors $\varphi$.
Final vector is a concatenation of columns of given matrix
\begin{equation*}
  \Lambda_v = \begin{bmatrix}
    1               & 0               & \dots & 0 \\
    \lambda^0_{v_x} & \lambda^1_{v_x} & \dots & \lambda^n_{v_x} \\
    \lambda^0_{v_y} & \lambda^1_{v_y} & \dots & \lambda^n_{v_y} \\
    \lambda^0_{v_z} & \lambda^1_{v_z} & \dots & \lambda^n_{v_z} \\
  \end{bmatrix}
  \mapsto
  \begin{bmatrix}
    1 \\
    \lambda^0_{v_x} \\
    \lambda^0_{v_y} \\
    \vdots \\
    \lambda^n_{v_y} \\
    \lambda^n_{v_z} \\
  \end{bmatrix}.
\end{equation*}
All these vectors can be concatenated to a matrix
in order to build a linear equation.
Example of the equation of $\varphi^y$
\begin{equation}\label{eq:linear:affine}
  \begin{bmatrix}
    1                 & 1                 & \dots & 1                 & \\
    \lambda^0_{v^1_x} & \lambda^0_{v^2_x} & \dots & \lambda^0_{v^m_x} & \\
    \lambda^0_{v^1_y} & \lambda^0_{v^2_y} & \dots & \lambda^0_{v^m_y} & \\
    \lambda^0_{v^1_z} & \lambda^0_{v^2_z} & \dots & \lambda^0_{v^m_z} & \\
    \vdots            & \vdots            & \dots & \vdots            & \\
    \lambda^n_{v^1_y} & \lambda^n_{v^2_y} & \dots & \lambda^n_{v^m_y} & \\
    \lambda^n_{v^1_z} & \lambda^n_{v^2_z} & \dots & \lambda^n_{v^m_z} & \\
  \end{bmatrix}
  \cdot
  \begin{bmatrix}
    \varphi^y_{v^1} \\
    \varphi^y_{v^2} \\
    \vdots          \\
    \varphi^y_{v^m} \\
  \end{bmatrix}
  =
  \begin{bmatrix}
    0       \\
    0       \\
    r_y     \\
    0       \\
    \vdots  \\
    0       \\
  \end{bmatrix}.
\end{equation}
where $r_y$ is a non-zero real number.
How to solve an equation if we don't know the value of $r_y$?
We can solve an equation which will give us a range of possible values of $r_y$
\begin{equation*}
  \begin{bmatrix}
    \lambda^0_{v^1_y} & \lambda^0_{v^2_y} & \dots & \lambda^0_{v^m_y} & \\
  \end{bmatrix}
  \cdot
  \begin{bmatrix}
    \varphi^y_{v^1} \\
    \varphi^y_{v^2} \\
    \vdots          \\
    \varphi^y_{v^m} \\
  \end{bmatrix}
  \neq 0,
\end{equation*}
and see what constraints vector $\varphi^y$ has.
Then the row which contains $\lambda^0_{v_y}$ coefficients
can be dropped from the original equation \eqref{eq:linear:affine}
and it can be solved as an ordinary linear equation
\begin{equation*}
  \begin{bmatrix}
    1                 & 1                 & \dots & 1                 & \\
    \lambda^0_{v^1_x} & \lambda^0_{v^2_x} & \dots & \lambda^0_{v^m_x} & \\
    \lambda^0_{v^1_z} & \lambda^0_{v^2_z} & \dots & \lambda^0_{v^m_z} & \\
    \vdots            & \vdots            & \dots & \vdots            & \\
    \lambda^n_{v^1_y} & \lambda^n_{v^2_y} & \dots & \lambda^n_{v^m_y} & \\
    \lambda^n_{v^1_z} & \lambda^n_{v^2_z} & \dots & \lambda^n_{v^m_z} & \\
  \end{bmatrix}
  \cdot
  \begin{bmatrix}
    \varphi^y_{v^1} \\
    \varphi^y_{v^2} \\
    \vdots          \\
    \varphi^y_{v^m} \\
  \end{bmatrix}
  =
  \begin{bmatrix}
    0       \\
    0       \\
    0       \\
    \vdots  \\
    0       \\
  \end{bmatrix}.
\end{equation*}
Intersection of solution of this equation
with constraints from the previous step
will be a final solution set, which may be empty.

When the system will be solved,
we will get two rows of the matrix $A$
and this will change bilinear equation \eqref{eq:bilinear:matrix}
to linear with respect to shape parameter $s$.
